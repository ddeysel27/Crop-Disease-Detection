{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203ec3c5",
   "metadata": {},
   "source": [
    "# 02 — Train PlantVillage Disease Classifier\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318015a2",
   "metadata": {},
   "source": [
    "## Overview — What This Notebook Does\n",
    "\n",
    "This notebook trains a **image classification model** (Vision Transformer) on the processed dataset.\n",
    "\n",
    "### Step-by-step workflow\n",
    "\n",
    "1. **Set paths and hyperparameters**  \n",
    "   - Point to the processed dataset, e.g. `data/processed/cassava`.  \n",
    "   - Define image size, batch size, learning rate, number of epochs, and device (CPU/GPU).\n",
    "\n",
    "2. **Define data transforms**  \n",
    "   - For **training**: apply resizing plus random augmentations (flips / rotations) to increase robustness and reduce overfitting.  \n",
    "   - For **validation**: apply only resizing and tensor conversion to keep evaluation consistent.\n",
    "\n",
    "3. **Create training and validation datasets**  \n",
    "   - Use `torchvision.datasets.ImageFolder` on:\n",
    "     - `.../train` → training images grouped in class folders  \n",
    "     - `.../val`   → validation images grouped in class folders  \n",
    "   - `ImageFolder` automatically builds:\n",
    "     - `classes` → list of class names  \n",
    "     - `class_to_idx` → mapping from class name to numeric label.\n",
    "\n",
    "4. **Wrap datasets in DataLoaders**  \n",
    "   - `DataLoader` batches images and shuffles the training data each epoch.  \n",
    "   - This gives us mini-batches like `(images, labels)` for efficient training on GPU/CPU.\n",
    "\n",
    "5. **Initialize the model (ViT)**  \n",
    "   - Load a pretrained **Vision Transformer** (`vit_b_16`) with ImageNet weights.  \n",
    "   - Replace the final classification head with a new `Linear` layer whose output size equals the number of disease classes in this dataset.  \n",
    "   - Move the model to the selected device (`cuda` if available).\n",
    "\n",
    "6. **Define loss function and optimizer**  \n",
    "   - Use **CrossEntropyLoss** for multi-class classification.  \n",
    "   - Use **Adam** optimizer with the chosen learning rate to update all model parameters.\n",
    "\n",
    "7. **Training loop (per epoch)**  \n",
    "   - Set the model to `train()` mode.  \n",
    "   - For each batch:\n",
    "     1. Move images and labels to the device.  \n",
    "     2. Do a forward pass to get predictions.  \n",
    "     3. Compute loss between predictions and true labels.  \n",
    "     4. Backpropagate (`loss.backward()`).  \n",
    "     5. Update weights (`optimizer.step()`).  \n",
    "     6. Zero the gradients for the next batch.  \n",
    "   - Track the average training loss for monitoring.\n",
    "\n",
    "8. **Validation after training (or per epoch)**  \n",
    "   - Set the model to `eval()` mode and disable gradients (`torch.no_grad()`).  \n",
    "   - Run the model on the validation DataLoader.  \n",
    "   - Compute overall accuracy by comparing predicted labels to true labels.  \n",
    "   - This tells us how well the model generalizes to unseen images.\n",
    "\n",
    "9. **Save the trained model**  \n",
    "   - Save the learned weights (e.g. `cassava_vit_best.pth`) so they can be reused later for:\n",
    "     - Inference notebooks  \n",
    "     - Comparison with other models  \n",
    "     - Deployment in a demo or chatbot.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7cb5e836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models, datasets\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0f2dc949",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/processed/plantVillage\"\n",
    "BATCH_SIZE = 32\n",
    "LR = 1e-4\n",
    "EPOCHS = 15\n",
    "IMG_SIZE = 224\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a862fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset_cards.md', 'interim', 'processed', 'raw']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir(\"../data\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eefbf0f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple___Apple_scab',\n",
       " 'Apple___Black_rot',\n",
       " 'Apple___Cedar_apple_rust',\n",
       " 'Apple___healthy',\n",
       " 'Blueberry___healthy',\n",
       " 'Cherry_(including_sour)___Powdery_mildew',\n",
       " 'Cherry_(including_sour)___healthy',\n",
       " 'Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot',\n",
       " 'Corn_(maize)___Common_rust_',\n",
       " 'Corn_(maize)___Northern_Leaf_Blight',\n",
       " 'Corn_(maize)___healthy',\n",
       " 'Grape___Black_rot',\n",
       " 'Grape___Esca_(Black_Measles)',\n",
       " 'Grape___Leaf_blight_(Isariopsis_Leaf_Spot)',\n",
       " 'Grape___healthy',\n",
       " 'Orange___Haunglongbing_(Citrus_greening)',\n",
       " 'Peach___Bacterial_spot',\n",
       " 'Peach___healthy',\n",
       " 'Pepper,_bell___Bacterial_spot',\n",
       " 'Pepper,_bell___healthy',\n",
       " 'Potato___Early_blight',\n",
       " 'Potato___Late_blight',\n",
       " 'Potato___healthy',\n",
       " 'Raspberry___healthy',\n",
       " 'Soybean___healthy',\n",
       " 'Squash___Powdery_mildew',\n",
       " 'Strawberry___Leaf_scorch',\n",
       " 'Strawberry___healthy',\n",
       " 'Tomato___Bacterial_spot',\n",
       " 'Tomato___Early_blight',\n",
       " 'Tomato___Late_blight',\n",
       " 'Tomato___Leaf_Mold',\n",
       " 'Tomato___Septoria_leaf_spot',\n",
       " 'Tomato___Spider_mites Two-spotted_spider_mite',\n",
       " 'Tomato___Target_Spot',\n",
       " 'Tomato___Tomato_Yellow_Leaf_Curl_Virus',\n",
       " 'Tomato___Tomato_mosaic_virus',\n",
       " 'Tomato___healthy']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "train_ds = datasets.ImageFolder(os.path.join(DATA_DIR, \"train\"), transform=train_tfms)\n",
    "val_ds   = datasets.ImageFolder(os.path.join(DATA_DIR, \"val\"),   transform=val_tfms)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "NUM_CLASSES = len(train_ds.classes)\n",
    "train_ds.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a707ac",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e16727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vit_b_16-c867db91.pth\" to C:\\Users\\User/.cache\\torch\\hub\\checkpoints\\vit_b_16-c867db91.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 330M/330M [04:56<00:00, 1.17MB/s] \n",
      "  1%|          | 8/1188 [03:45<9:18:05, 28.38s/it]"
     ]
    }
   ],
   "source": [
    "model = models.vit_b_16(weights=\"IMAGENET1K_V1\")\n",
    "model.heads.head = nn.Linear(model.heads.head.in_features, NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=LR)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_dl):\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{EPOCHS} | Train Loss: {total_loss/len(train_dl):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bcdbdc",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4579b633",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct, total = 0, 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, labels in val_dl:\n",
    "        imgs, labels = imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        preds = model(imgs).argmax(1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "accuracy = correct / total\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf777fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(model.state_dict(), \"cassava_vit_best.pth\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
